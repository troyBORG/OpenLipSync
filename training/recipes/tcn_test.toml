# OpenLipSync TCN Model Configuration
# All tunable parameters for the Temporal Convolutional Network (TCN)
#
# TCN = Temporal Convolutional Network: a type of neural network designed for
# sequence modeling that can process audio/video over time while maintaining
# low latency (minimal delay between input and output)

[model]
# Model architecture - these parameters control the "shape" and capacity of your neural network
name = "tcn"

# Number of output classes (what the model predicts)
num_visemes = 15  # CRITICAL: Must match the number of unique visemes in phoneme_viseme_map
# More classes = more detailed lip sync, but harder to train


# Network depth and capacity
layers = 5  # Number of layers in the network
# More layers = model can learn more complex patterns, but:
# - Slower training/inference
# - Needs more data to avoid overfitting
# - Longer "memory" (receptive field)

channels = 128  # Width of each layer (number of features/neurons)
# More channels = model can represent more complex features, but:
# - Uses more memory and computation
# - May overfit on small datasets
# Common values: 32, 64, 128, 256

kernel_size = 3  # Size of convolutional filters
# Larger kernels = model sees more context at once

# Regularization - prevents overfitting (model memorizing training data)
dropout = 0.1  # Randomly "turn off" 10% of neurons during training
# Higher dropout = more regularization, but may hurt learning
# Typical range: 0.0-0.3

normalization = "weight_norm"  # How to normalize layer inputs
# Options: "weight_norm", "layer_norm", "batch_norm"
# Helps training stability and convergence


[audio]
# Audio processing parameters - how we convert raw audio into features the model can understand

# Sample rate = how many audio samples per second
sample_rate = 16000  # 16kHz is good for speech (phones use 8kHz, music uses 44kHz+)
# Lower sample rate = less detail but faster processing


hop_length_ms = 10  # How often we analyze the audio (10ms for 100fps)
window_length_ms = 25  # How much audio we look at each time (25ms window)
# Window > hop means overlapping analysis windows (good for smooth features)

# LOG-MEL SPECTROGRAM: Converts audio waveform into a 2D image-like representation
# Think of it as a "visual" representation of sound frequencies over time
n_mels = 80  # Number of frequency bands (80 is standard for speech)
# More bands = more frequency detail, but slower processing
# 64-100 is typical range

fmin = 50   # Lowest frequency we care about (Hz)
fmax = 6000 # Focus on formant-rich regions (better for lip-sync)
# Most lip-sync relevant info is below 6kHz
# Reduces noise from high-frequency artifacts

n_fft = 1024  # FFT size for frequency analysis
# Larger = better frequency resolution, smaller = better time resolution
# Should be power of 2 (512, 1024, 2048, etc.)

normalization = "none"  # Keep raw mel spectrogram values
# per_utterance: Standardize each file (can hurt viseme distinction)  
# global: Dataset-wide standardization (not implemented)
# none: Preserve natural energy patterns (often better for lip-sync)

[training]
# Training parameters - how the model learns from data

# BATCH SIZE: How many audio clips to process together
batch_size = 32
# Larger batches = more stable gradients, but need more GPU memory
# Smaller batches = noisier but sometimes better generalization
# Common values: 16, 32, 64 (powers of 2 work best)

# Audio chunking - long audio files are split into smaller pieces
max_chunk_length_s = 16.0  # Shorter chunks for better local patterns
min_chunk_length_s = 1.0  # Shorter minimum for more variety
# Shorter chunks = faster training, but less context
# Longer chunks = more context, but slower and more memory

mixed_precision = false  # Use 16-bit instead of 32-bit numbers
# This speeds up training and saves memory with minimal accuracy loss
# Modern GPUs (RTX 20xx+) support this well

# LOSS FUNCTION: How we measure prediction errors
loss_type = "focal_loss"  # Focal loss handles imbalance better than extreme weighting
# Options: "cross_entropy" (standard or multi-label via BCE), "focal_loss" (for imbalanced data)
multi_label = false            # Enable multi-label training (sigmoid + BCE); targets become multi-hot per frame
target_crossfade_ms = 0      # Soft multi-hot targets around boundaries (0 disables). Use ~20–60ms
 
# OVERLAP: allow multiple visemes to be considered active near boundaries (also used by metrics)
viseme_overlap_enabled = false  # Allow overlapping visemes in evaluation
viseme_overlap_threshold = 0.0 # Threshold for considering visemes as overlapping

class_weighting = false  # Disable extreme class weighting (was causing instability)
# Some mouth shapes are much rarer than others in speech
# This prevents the model from ignoring rare but important visemes

# SILENCE BIAS: Prefer neutral mouth over incorrect movements
silence_bias = 0.1  # Add bias towards silence class (0.0 = no bias, 0.5 = strong bias)
# Helps avoid false lip movements during pauses and uncertain regions
silence_energy_gate_db = -60.0     # If avg frame energy < this (approx dB), boost silence further
silence_energy_gate_bias = 0.2     # Extra bias added to silence under the gate

# Focal loss parameters (only used if loss_type = "focal_loss")
focal_loss_alpha = 1.0  # Class balancing factor, 
focal_loss_gamma = 2.0  # Focus on hard examples 
# Focal loss helps when some classes are much rarer than others

# OPTIMIZER: The algorithm that updates model weights
optimizer = "adamw"  # AdamW is a good default for many problems
learning_rate = 3e-4  # How big steps to take when learning (0.0003)
# Too high = unstable training, too low = very slow learning
# 3e-4 is a good starting point for most models

betas = [0.9, 0.98]  # Momentum parameters for AdamW
# Controls how much to use previous gradients vs current gradient
# [0.9, 0.999] is standard, [0.9, 0.98] sometimes better for transformers

weight_decay = 1e-2  # L2 regularization strength (0.01)
# Prevents weights from getting too large (helps avoid overfitting)
# Higher values = more regularization

# LEARNING RATE SCHEDULER: Changes learning rate during training
scheduler = "cosine" 
# Options: 
#"onecycle" (fast)
# "cosine" (smooth)
#"constant" (no change)
warmup_ratio = 0.05  # Gradually increase LR for first 5% of training
# Warmup helps training stability at the beginning

max_epochs = 100  # Maximum number of times to see the full dataset
# Training will stop early if performance stops improving

# EARLY STOPPING: Stop training when performance plateaus
early_stopping_patience = 10  # Stop if no improvement for 5 epochs
early_stopping_metric = "val_f1"  # What metric to watch
# Options: "val_loss" (validation loss), "val_f1" (F1 score)
# Prevents overfitting and saves time

# DATA AUGMENTATION: Slightly modify training data to improve generalization
specaugment_enabled = true  # Randomly mask parts of the spectrogram
specaugment_time_mask_max_ms = 20  # Max audio to mask
# This teaches the model to handle missing/corrupted audio segments

mask_padded_frames = false  # Mask padded frames in loss calculation


[data]
# Data pipeline parameters - what data to use and how to process it

# DATASET: LibriSpeech is a large, free speech dataset
dataset = "librispeech"

#splits = ["test-clean"] #For quick testing

#splits = ["train-clean-100"] #For training


#splits = ["train-clean-360"] #For training, slow
splits = ["train-clean-100","train-clean-360"] #For training, slow
#splits = ["train-clean-100", "train-clean-360", "train-other-500"] #For training, very slow
#splits = ["train-other-500"] #For training, very slow


# train-clean-100: [~6GB] 100h of clean speech (start here for testing)
# train-clean-360: [~23GB] 360h of clean speech
# train-other-500: [~30GB] 500h of varied/noisy speech

#In special for 360 and 500 grab a coffee(or your favorite beverage) because it will take a while.


val_split = "dev-clean"    # Validation set (for monitoring training)
test_split = "test-clean"  # Test set (for final evaluation only)
# Never train on validation or test data!

# OPTIONAL DATA AUGMENTATION: Make training data more varied
augmentation_enabled = true  # Enable for silence/noise augmentation and SNR/gain
noise_snr_range = [10, 30]   # Add noise at 10-30 dB signal-to-noise ratio
gain_range = [0.8, 1.2]     # Randomly change volume by ±20%
# Augmentation helps the model work in noisy real-world conditions
# But can slow training, so start without it

# Synthetic silence/near-silence augmentation to reduce false activations on silence
silence_augment_prob = 0.15         # ~15% extra synthetic near-silence items added
silence_noise_dbfs_range = [-65.0, -40.0]  # Noise level for near-silence
silence_chunk_length_s = [1.0, 4.0]       # Duration range for synthetic silence chunks

# PHONEME TO VISEME MAPPING: How speech sounds map to mouth shapes
phoneme_viseme_map = "training/configs/viseme_map_en_us_arpa.json"
# This file defines which mouth shape to use for each speech sound
# Different languages/accents might need different mappings
#
# ⚠️  CRITICAL: The ORDER of visemes in this JSON file determines class indices!
# The model learns: 0=silence, 1=PP, 2=FF, 3=TH, 4=DD, 5=kk, 6=CH, 7=SS, 8=nn, 9=RR, 10=aa, 11=E, 12=ih, 13=oh, 14=ou
# 
# If you change the viseme order, you MUST retrain all models from scratch.
# If you add/remove visemes, update num_visemes above and retrain models.
# Always backup this file before making any changes!

[evaluation]
# Evaluation metrics - how we measure if the model is working well

# What metrics to calculate during training/testing
metrics = ["frame_accuracy", "macro_f1", "confusion_matrix"]
# frame_accuracy: % of frames predicted correctly (simple but can be misleading)
# macro_f1: Average F1 score across all viseme classes (better for imbalanced data)
# confusion_matrix: Shows which visemes get confused with each other

compute_latency = true        # Measure how fast the model runs
target_hardware = "cpu"      # Test speed on CPU (for real-time requirements)
# Real-time = model must process audio faster than it arrives
# RTF (Real-Time Factor) < 1.0 means faster than real-time

# Cross-fade is controlled by training.target_crossfade_ms

[hardware]
# Hardware-specific settings - configure for your computer

device = "cpu"      # Target device for training
# Options: "cuda" (NVIDIA GPU or AMD GPU with ROCm), "cpu" (slow but works everywhere), "mps" (Apple Silicon)
# Check: nvidia-smi (for CUDA), rocm-smi (for ROCm/AMD), or check GPU in system settings

num_workers = 8      # How many CPU cores to use for loading data
# More workers = faster data loading, but uses more CPU/memory
# Good starting point: number of CPU cores / 2

pin_memory = false    # Speed optimization for GPU training
# Keeps data in fast memory for GPU transfer (use true if you have enough RAM)

[logging]
# Logging and checkpointing - track progress and save models

log_interval = 20    # Print training stats every X steps
save_interval = 500  # Save model checkpoint every X steps
# More frequent logging = better monitoring, but slower training

max_checkpoints = 10   # Keep only the 5 most recent model saves
# Prevents disk from filling up with old models
# The best model (lowest validation loss) is always kept

log_level = "INFO"    # How much detail to print
# Options: "DEBUG" (everything), "INFO" (normal), "WARNING" (problems only)

[tensorboard]
# TensorBoard logging configuration - visualize training progress

enabled = true                    # Enable TensorBoard logging
runs_dir = "training/runs"        # Directory to store TensorBoard logs
# TensorBoard files will be saved to: runs_dir/run_name/

# Run naming configuration - automatically generate unique run names
run_name_format = "{experiment_name}_{timestamp}"  # How to name each run
# Available variables:
# - {experiment_name}: from [experiment] section
# - {timestamp}: current date/time (YYYY-MM-DD_HH-MM-SS)
# - {epoch}: current epoch (useful for resuming)
# - {lr}: learning rate
# - {batch_size}: batch size

# Manual run name override (optional)
# run_name = "custom_run_name"    # Uncomment to use a specific name instead of format

# What to log to TensorBoard
log_scalars = true              # Loss, accuracy, learning rate, etc.
log_histograms = false          # Weight/gradient distributions (slower but detailed)
log_images = true              # Spectrograms and confusion matrices (uses disk space)
log_audio = false               # Audio samples (useful for debugging, uses lots of space)

# Update frequency
scalar_log_interval = 20        # Log scalars every N steps (matches log_interval)
histogram_log_interval = 100    # Log histograms every N steps (if enabled)
image_log_interval = 500        # Log images every N steps (if enabled)

[experiment]
# Experiment tracking - organize your training runs

name = "audio_augmentation_full_dataset4"                              # Name for this experiment
tags = ["tcn", "viseme", "100fps"]               # Tags to find related experiments
notes = "TCN model test run with audio augmentation"  # Description of what you're testing

# TIP: Change the name for each experiment so you can compare results
# Example names: "tcn_more_layers", "tcn_50fps", "tcn_focal_loss"
